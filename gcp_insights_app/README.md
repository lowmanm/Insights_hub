# GCP‑Based Fulfilment Insights Web App

This repository contains a skeleton for a data‑analytics and insights web
application designed for fulfilment operations.  The goal of this project
is to provide operational leaders with a single hub for understanding
performance and trends without constantly writing ad‑hoc SQL or manually
building dashboards.  Two core components drive the application:

1. **Insights Hub** – A dashboard that surfaces key metrics from your BigQuery
   warehouse such as prior‑day performance, month‑to‑date metrics and
   predictions for the current day.  The hub is intended to be a quick
   landing page for leaders who need high‑level information at a glance.
2. **Insights Bot** – A conversational interface that allows users to
   query the underlying data warehouse in natural language.  As a user
   refines their question the bot will guide them towards the right
   filters and aggregations, execute queries against BigQuery on your
   behalf, and optionally generate a report summarising the findings.

The tech stack for this project has been chosen to align with a typical
Google Cloud Platform environment.  The backend uses **Django** with
Google’s Python libraries to query BigQuery, while the frontend uses
**Angular** and vanilla JavaScript via CDNs for ease of setup and
deployment.  Infrastructure as code (Terraform) and workflow
orchestration (Airflow/Composer) can be layered on in future commits.

## Getting started

1. **Clone this repository**

   ```bash
   git clone <this‑repository‑url>
   cd gcp_insights_app
   ```

2. **Back‑end setup**

   The backend is a standard Django project.  It exposes REST endpoints
   for the insights hub and the chat bot.  It reads your BigQuery
   credentials from environment variables.

   ```bash
   cd backend
   python3 -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt

   # copy the sample environment file and edit it with your values
   cp .env.sample .env

   # run migrations and start the development server
   python manage.py migrate
   python manage.py runserver 0.0.0.0:8000
   ```

   The API will be available at `http://localhost:8000/` and the health
   check at `/api/ping/`.

3. **Front‑end setup**

   A minimal Angular‑based frontend is provided in the `frontend/`
   directory.  No build step is required; all dependencies are loaded
   from CDNs.  You can simply open `frontend/index.html` in your
   browser or serve it using any static web server.  To allow the
   frontend to communicate with your backend running on port 8000, you
   may want to serve the frontend via Python for local development:

   ```bash
   # from the repository root
   python3 -m http.server --directory frontend 3000
   ```

   The user interface currently demonstrates a very basic layout for the
   insights hub and integrates a simple chat form that sends requests to
   the bot endpoint.  You can start customising and expanding the UI as
   needed.

4. **Deployment**

   Deployment scripts (Terraform, Docker, Cloud Build) are not included
   yet.  You can choose to run this application on Cloud Run, App
   Engine, Compute Engine or Kubernetes.  The use of CDNs for
   JavaScript and CSS means you do not need a Node build pipeline for
   the frontend, simplifying container builds.

## Authentication, security and privacy

The application connects to your organisation’s BigQuery datasets and
Vertex AI services through a service account specified via environment
variables.  You should create a dedicated service account with the
minimum IAM roles required for reading BigQuery tables and calling
Vertex AI.  The chat bot uses Vertex AI’s generative models (via the
`google‑cloud‑aiplatform` library) and can optionally leverage
BigQuery ML models to generate predictions.

Authentication to the web application itself is handled by Saviynt
using **Scramble Id**.  The backend includes a stubbed
`SaviyntAuthentication` class that extracts a `X‑Scramble‑Id` header
from incoming requests and uses it as the username.  In your
production deployment you will need to replace this stub with calls
into Saviynt’s API to validate tokens, map entitlements and load user
metadata.  Only authenticated and entitled users should be allowed to
access the insights hub and chat endpoints.

**Important:** All data fetched from BigQuery and all responses
generated by Vertex AI stay within your organisation’s environment.  The
models powering the chat bot are invoked via your GCP project and
configured not to use your data for public training.  You are
responsible for configuring appropriate IAM roles, service account
scopes and Vertex AI settings to enforce this.

## Roadmap

This repository currently provides a skeleton and a starting point.  The
following items are candidates for future development:

* **Authentication and authorisation:** integrate with Google Identity
  Platform or Firebase Authentication to restrict access to authorised
  leaders.
* **Airflow/Composer DAGs:** build DAGs to schedule nightly and
  intraday metrics computation and store results in BigQuery.
* **Scheduled triggers:** use Pub/Sub and Cloud Functions/Dataflow for
  streaming predictions to the insights hub.
* **Dashboards:** expand the insights hub with charts, tables and
  interactive filters using libraries like `ng2-charts` or
  `highcharts-angular`.
* **Natural language understanding:** connect the chat bot to a
  foundation model such as Google Vertex AI’s PaLM for natural
  conversation, while using Retrieval Augmented Generation (RAG) to
  securely query your BigQuery data.
